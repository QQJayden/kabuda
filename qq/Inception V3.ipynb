{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 如何修改误差为AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2022, 224, 224, 3) (2022,) (662, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load('./data/train.npy')\n",
    "label_data = np.load('./data/label.npy')\n",
    "test_data = np.load('./data/test.npy')\n",
    "filename_data = np.load('./data/filename.npy')\n",
    "\n",
    "print(train_data.shape,label_data.shape,test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2022, 224, 224, 3) (662, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "target_train = label_data\n",
    "X_train = train_data\n",
    "X_test = test_data\n",
    "\n",
    "# for i in range(2022):\n",
    "#     img_temp_train = cv2.resize(train_data[i],(197,197)).astype(np.float32)\n",
    "#     X_train0.append(img_temp_train)\n",
    "    \n",
    "# for i in range(662):\n",
    "#     img_temp_test = cv2.resize(test_data[i],(197,197)).astype(np.float32)\n",
    "#     X_test0.append(img_temp_test)\n",
    "    \n",
    "# X_train0 = np.array(X_train0)\n",
    "# X_test0 = np.array(X_test0)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 每张图像归一化\n",
    "X_train = []\n",
    "X_test = []\n",
    "\n",
    "for img in X_train0:\n",
    "    r = (img[:,:,0]-np.mean(img[:,:,0]))/np.std(img[:,:,0])\n",
    "    g = (img[:,:,1]-np.mean(img[:,:,1]))/np.std(img[:,:,1])\n",
    "    b = (img[:,:,2]-np.mean(img[:,:,2]))/np.std(img[:,:,2])\n",
    "    \n",
    "    rgb = np.dstack((r,g,b))\n",
    "    X_train.append(rgb)\n",
    "    \n",
    "for img in X_test0:\n",
    "    r = (img[:,:,0]-np.mean(img[:,:,0]))/np.std(img[:,:,0])\n",
    "    g = (img[:,:,1]-np.mean(img[:,:,1]))/np.std(img[:,:,1])\n",
    "    b = (img[:,:,2]-np.mean(img[:,:,2]))/np.std(img[:,:,2])\n",
    "    \n",
    "    rgb = np.dstack((r,g,b))\n",
    "    X_test.append(rgb)\n",
    "    \n",
    "X_train = np.array(X_train)\n",
    "X_test=np.array(X_test)\n",
    "    \n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import GlobalMaxPooling2D, Dense, BatchNormalization, GlobalAveragePooling2D, Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.optimizers import Adamax\n",
    "from keras.optimizers import Nadam\n",
    "\n",
    "#Create the model\n",
    "\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# x = Dense(1024, activation='relu')(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Dense(256, activation='relu')(x)\n",
    "# # x = Dropout(0.5)(x)\n",
    "# x = Dropout(0.3)(x)\n",
    "\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "#for layer in base_model.layers:\n",
    "#    layer.trainable = False\n",
    "for layer in model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[15:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用不同的优化\n",
    "sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adagrad = Adagrad(lr = 1e-3, epsilon = 1e-6)\n",
    "rmsprop = RMSprop(lr=1e-3, rho = 0.9, epsilon=1e-6)\n",
    "adadelta = Adadelta(lr=1e-3, rho=0.95, epsilon=1e-06)\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "adamax = Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "nadam = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "\n",
    "\n",
    "# 自定义损失函数\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "# FROM https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/41108\n",
    "def jacek_auc(y_true, y_pred):\n",
    "#     score, up_opt = tf.metrics.auc(y_true, y_pred)\n",
    "    score, up_opt = tf.contrib.metrics.streaming_auc(y_pred, y_true)    \n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    with tf.control_dependencies([up_opt]):\n",
    "        score = tf.identity(score)\n",
    "    return score\n",
    "\n",
    "model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "batch_size = 16 # 原来是3\n",
    "\n",
    "#Lets define the image transormations that we want\n",
    "gen = ImageDataGenerator(horizontal_flip=True,\n",
    "                         vertical_flip=True,\n",
    "                         width_shift_range=0.,\n",
    "                         height_shift_range=0.,\n",
    "                         zoom_range=0.2,\n",
    "                         rotation_range=10)\n",
    "\n",
    "# Here is the function that merges our two generators\n",
    "# We use the exact same generator with the same random seed for both the y and angle arrays\n",
    "def gen_flow_for_one_input(X1, y):\n",
    "    genX1 = gen.flow(X1,y,  batch_size=batch_size,seed=55)\n",
    "    while True:\n",
    "            X1i = genX1.next()\n",
    "            #Assert arrays are equal - this was for peace of mind, but slows down training\n",
    "            #np.testing.assert_array_equal(X1i[0],X2i[0])\n",
    "            yield X1i[0], X1i[1]\n",
    "\n",
    "#Finally create out generator\n",
    "# gen_flow = gen_flow_for_one_inputs(X_train, y_train)\n",
    "\n",
    "# Finally create generator\n",
    "def get_callbacks(filepath, patience=2):\n",
    "   es = EarlyStopping('val_loss', patience=10, mode=\"min\")\n",
    "   msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "   return [es, msave]\n",
    "\n",
    "'''\n",
    "epochs_to_wait_for_improve = 10\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=epochs_to_wait_for_improve)\n",
    "checkpoint_callback = ModelCheckpoint('./model/BestKerasModelResNet50.h5', monitor='val_loss', \n",
    "                                      verbose=1, save_best_only=True, mode='min')\n",
    "'''\n",
    "\n",
    "#Using K-fold Cross Validation with Data Augmentation.\n",
    "def mytrainCV(X_train, X_test):\n",
    "    # K-折交叉验证\n",
    "    K=3\n",
    "    \n",
    "    folds = list(StratifiedKFold(n_splits=K, shuffle=True, random_state=16).split(X_train, target_train))\n",
    "    y_test_pred_log = 0\n",
    "    y_train_pred_log=0\n",
    "    y_valid_pred_log = 0.0*target_train\n",
    "    \n",
    "    auc = 0;\n",
    "    \n",
    "    for j, (train_idx, test_idx) in enumerate(folds):\n",
    "        print('\\n===================FOLD=',j)\n",
    "        X_train_cv = X_train[train_idx]\n",
    "        y_train_cv = target_train[train_idx]\n",
    "        X_holdout = X_train[test_idx]\n",
    "        Y_holdout= target_train[test_idx]\n",
    "        \n",
    "\n",
    "        #define file path and get callbacks\n",
    "        file_path = \"./model/%s_aug_IncepV3_model_weights.hdf5\"%j\n",
    "        callbacks = get_callbacks(filepath=file_path, patience=5)\n",
    "        gen_flow = gen_flow_for_one_input(X_train_cv, y_train_cv)\n",
    "\n",
    "        galaxyModel= model\n",
    "    \n",
    "        # 调整训练参数\n",
    "        galaxyModel.fit_generator(\n",
    "                gen_flow,\n",
    "#                 steps_per_epoch=24,\n",
    "                steps_per_epoch=len(X_train_cv)//batch_size,\n",
    "                epochs=100,\n",
    "                shuffle=True,\n",
    "                verbose=1,\n",
    "                validation_data=(X_holdout, Y_holdout),\n",
    "                callbacks=callbacks)\n",
    "\n",
    "        #Getting the Best Model\n",
    "        galaxyModel.load_weights(filepath=file_path)\n",
    "        \n",
    "        #Getting Training Score\n",
    "        score = galaxyModel.evaluate(X_train_cv, y_train_cv, verbose=0)\n",
    "        print('Train loss:', score[0])\n",
    "        print('Train accuracy:', score[1])\n",
    "        \n",
    "        #Getting Test Score\n",
    "        score = galaxyModel.evaluate(X_holdout, Y_holdout, verbose=0)\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "\n",
    "        #Getting validation Score.       \n",
    "        pred_valid=galaxyModel.predict(X_holdout)\n",
    "        y_valid_pred_log[test_idx] = pred_valid.reshape(pred_valid.shape[0])\n",
    "\n",
    "        #Getting Test Scores\n",
    "        temp_test=galaxyModel.predict(X_test)\n",
    "        y_test_pred_log+=temp_test.reshape(temp_test.shape[0])\n",
    "\n",
    "        #Getting Train Scores        \n",
    "        temp_train=galaxyModel.predict(X_train)\n",
    "        y_train_pred_log+=temp_train.reshape(temp_train.shape[0])\n",
    "        \n",
    "        # AUC \n",
    "        auc_temp = roc_auc_score(Y_holdout,pred_valid)\n",
    "        print(\"AUC = {0:0.4f}\".format(auc_temp))\n",
    "        \n",
    "        auc+=auc_temp\n",
    "        \n",
    "    y_test_pred_log=y_test_pred_log/K\n",
    "    y_train_pred_log=y_train_pred_log/K\n",
    "    auc = auc/K\n",
    "\n",
    "    print('\\n Train Loss Validation= ',log_loss(target_train, y_train_pred_log))\n",
    "    print(' Test Loss Validation= ',log_loss(target_train, y_valid_pred_log))\n",
    "    print('AUC Validation=',auc)\n",
    "    return y_test_pred_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================FOLD= 0\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 57s - loss: 0.6620 - acc: 0.6399 - val_loss: 0.6029 - val_acc: 0.7156\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 35s - loss: 0.5899 - acc: 0.7001 - val_loss: 0.5802 - val_acc: 0.7126\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 35s - loss: 0.5450 - acc: 0.7440 - val_loss: 0.5494 - val_acc: 0.7304\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 35s - loss: 0.5413 - acc: 0.7490 - val_loss: 0.5502 - val_acc: 0.7393\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 36s - loss: 0.5321 - acc: 0.7406 - val_loss: 0.5422 - val_acc: 0.7407\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 36s - loss: 0.5118 - acc: 0.7730 - val_loss: 0.5304 - val_acc: 0.7467\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 35s - loss: 0.4900 - acc: 0.7728 - val_loss: 0.5719 - val_acc: 0.7185\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 35s - loss: 0.4993 - acc: 0.7699 - val_loss: 0.5421 - val_acc: 0.7437\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 36s - loss: 0.4804 - acc: 0.7763 - val_loss: 0.5271 - val_acc: 0.7674\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 35s - loss: 0.4849 - acc: 0.7780 - val_loss: 0.5169 - val_acc: 0.7600\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 35s - loss: 0.4542 - acc: 0.8021 - val_loss: 0.5179 - val_acc: 0.7644\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 34s - loss: 0.4549 - acc: 0.7870 - val_loss: 0.5251 - val_acc: 0.7807\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 35s - loss: 0.4586 - acc: 0.7937 - val_loss: 0.5295 - val_acc: 0.7719\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 34s - loss: 0.4407 - acc: 0.7907 - val_loss: 0.5471 - val_acc: 0.7556\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 35s - loss: 0.4367 - acc: 0.8095 - val_loss: 0.5373 - val_acc: 0.7585\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 34s - loss: 0.4051 - acc: 0.8205 - val_loss: 0.5362 - val_acc: 0.7704\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 35s - loss: 0.4334 - acc: 0.8011 - val_loss: 0.5642 - val_acc: 0.7689\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 34s - loss: 0.4223 - acc: 0.8197 - val_loss: 0.5678 - val_acc: 0.7556\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 34s - loss: 0.4126 - acc: 0.8096 - val_loss: 0.5978 - val_acc: 0.7481\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 35s - loss: 0.4140 - acc: 0.8065 - val_loss: 0.5974 - val_acc: 0.7393\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 35s - loss: 0.3919 - acc: 0.8361 - val_loss: 0.5663 - val_acc: 0.7481\n",
      "Train loss: 0.4045116350928677\n",
      "Train accuracy: 0.8240534521600629\n",
      "Test loss: 0.5168793091509077\n",
      "Test accuracy: 0.7600000000883032\n",
      "AUC = 0.7836\n",
      "\n",
      "===================FOLD= 1\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 37s - loss: 0.4794 - acc: 0.7671 - val_loss: 0.4599 - val_acc: 0.7908\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 37s - loss: 0.4696 - acc: 0.7902 - val_loss: 0.4557 - val_acc: 0.7893\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 34s - loss: 0.4642 - acc: 0.8050 - val_loss: 0.4797 - val_acc: 0.7834\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 35s - loss: 0.4369 - acc: 0.8110 - val_loss: 0.4736 - val_acc: 0.7878\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 34s - loss: 0.4774 - acc: 0.7783 - val_loss: 0.4886 - val_acc: 0.7700\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 35s - loss: 0.4422 - acc: 0.8065 - val_loss: 0.4976 - val_acc: 0.7849\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 35s - loss: 0.4297 - acc: 0.8229 - val_loss: 0.4846 - val_acc: 0.7878\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 35s - loss: 0.4578 - acc: 0.7946 - val_loss: 0.4721 - val_acc: 0.7923\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 35s - loss: 0.3867 - acc: 0.8363 - val_loss: 0.4866 - val_acc: 0.7953\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 35s - loss: 0.4250 - acc: 0.8118 - val_loss: 0.5291 - val_acc: 0.7789\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 35s - loss: 0.4022 - acc: 0.8229 - val_loss: 0.5260 - val_acc: 0.7730\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 35s - loss: 0.4122 - acc: 0.8311 - val_loss: 0.5031 - val_acc: 0.7923\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 35s - loss: 0.4012 - acc: 0.8229 - val_loss: 0.5075 - val_acc: 0.7760\n",
      "Train loss: 0.42714665268578234\n",
      "Train accuracy: 0.8137982195845698\n",
      "Test loss: 0.4557311384543467\n",
      "Test accuracy: 0.7893175074183977\n",
      "AUC = 0.8477\n",
      "\n",
      "===================FOLD= 2\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 36s - loss: 0.5081 - acc: 0.7641 - val_loss: 0.3788 - val_acc: 0.8425\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 36s - loss: 0.4946 - acc: 0.7708 - val_loss: 0.3669 - val_acc: 0.8440\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 35s - loss: 0.4994 - acc: 0.7713 - val_loss: 0.3927 - val_acc: 0.8425\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 35s - loss: 0.4627 - acc: 0.7906 - val_loss: 0.3832 - val_acc: 0.8306\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 35s - loss: 0.4755 - acc: 0.7854 - val_loss: 0.4014 - val_acc: 0.8083\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 35s - loss: 0.4424 - acc: 0.8036 - val_loss: 0.4153 - val_acc: 0.8217\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 35s - loss: 0.4525 - acc: 0.7973 - val_loss: 0.4093 - val_acc: 0.8217\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 35s - loss: 0.4142 - acc: 0.8220 - val_loss: 0.4042 - val_acc: 0.8232\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 35s - loss: 0.4211 - acc: 0.8125 - val_loss: 0.4292 - val_acc: 0.8232\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 35s - loss: 0.4503 - acc: 0.8012 - val_loss: 0.4027 - val_acc: 0.8187\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 35s - loss: 0.4546 - acc: 0.7906 - val_loss: 0.4214 - val_acc: 0.8158\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 35s - loss: 0.4178 - acc: 0.8086 - val_loss: 0.4198 - val_acc: 0.8247\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 35s - loss: 0.4257 - acc: 0.8033 - val_loss: 0.4411 - val_acc: 0.8053\n",
      "Train loss: 0.4394218630928566\n",
      "Train accuracy: 0.8035581913411485\n",
      "Test loss: 0.3668979005099317\n",
      "Test accuracy: 0.8439821693907875\n",
      "AUC = 0.9001\n",
      "\n",
      " Train Loss Validation=  0.41774209499687076\n",
      " Test Loss Validation=  0.4465769598318529\n",
      "AUC Validation= 0.8437988301028579\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "preds=mytrainCV(X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Submission for each day.\n",
    "submission = pd.DataFrame()\n",
    "submission['filename']=filename_data\n",
    "submission['probability']=preds\n",
    "# submission.to_csv('./submission/subTLResNet1.5.csv', index=False)\n",
    "submission.to_csv('./submission/subIncepV3-2.0.csv', \n",
    "                  float_format='%.6f',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
