{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 如何修改误差为AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2022, 224, 224, 3) (2022,) (662, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load('./data/train.npy')\n",
    "label_data = np.load('./data/label.npy')\n",
    "test_data = np.load('./data/test.npy')\n",
    "filename_data = np.load('./data/filename.npy')\n",
    "\n",
    "print(train_data.shape,label_data.shape,test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0].shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import cv2\n",
    "\n",
    "target_train = label_data\n",
    "X_train = train_data\n",
    "X_test = test_data\n",
    "\n",
    "for i in range(2022):\n",
    "    img_temp_train = cv2.resize(train_data[i],(150,150)).astype(np.int8)\n",
    "    X_train0.append(img_temp_train)\n",
    "    \n",
    "for i in range(662):\n",
    "    img_temp_test = cv2.resize(test_data[i],(150,150)).astype(np.int8)\n",
    "    X_test0.append(img_temp_test)\n",
    "    \n",
    "X_train0=np.array(X_train0)\n",
    "X_test0=np.array(X_test0)\n",
    "\n",
    "print(X_train0.shape,X_test0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2022, 224, 224, 3) (662, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# 每张图像归一化\n",
    "target_train = label_data\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "\n",
    "for img in train_data:\n",
    "    r = (img[:,:,0]-np.mean(img[:,:,0]))/np.std(img[:,:,0])\n",
    "    g = (img[:,:,1]-np.mean(img[:,:,1]))/np.std(img[:,:,1])\n",
    "    b = (img[:,:,2]-np.mean(img[:,:,2]))/np.std(img[:,:,2])\n",
    "    \n",
    "    rgb = np.dstack((r,g,b))\n",
    "    X_train.append(rgb)\n",
    "    \n",
    "for img in test_data:\n",
    "    r = (img[:,:,0]-np.mean(img[:,:,0]))/np.std(img[:,:,0])\n",
    "    g = (img[:,:,1]-np.mean(img[:,:,1]))/np.std(img[:,:,1])\n",
    "    b = (img[:,:,2]-np.mean(img[:,:,2]))/np.std(img[:,:,2])\n",
    "    \n",
    "    rgb = np.dstack((r,g,b))\n",
    "    X_test.append(rgb)\n",
    "    \n",
    "X_train = np.array(X_train)\n",
    "X_test=np.array(X_test)\n",
    "    \n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "# Memory error\n",
    "# for n in X_train:\n",
    "#     X_train[n,:,:,0] = (X_train[n,:,:,0]-np.mean(X_train[n,:,:,0]))/np.std(X_train[n,:,:,0])\n",
    "#     X_train[n,:,:,1] = (X_train[n,:,:,1]-np.mean(X_train[n,:,:,1]))/np.std(X_train[n,:,:,1])\n",
    "#     X_train[n,:,:,2] = (X_train[n,:,:,2]-np.mean(X_train[n,:,:,2]))/np.std(X_train[n,:,:,2])\n",
    "    \n",
    "# for n in X_test:\n",
    "#     X_test[n,:,:,0] = (X_test[n,:,:,0]-np.mean(X_test[n,:,:,0]))/np.std(X_test[n,:,:,0])\n",
    "#     X_test[n,:,:,1] = (X_test[n,:,:,1]-np.mean(X_test[n,:,:,1]))/np.std(X_test[n,:,:,1])\n",
    "#     X_test[n,:,:,2] = (X_test[n,:,:,2]-np.mean(X_test[n,:,:,2]))/np.std(X_test[n,:,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.layers import GlobalMaxPooling2D, Dense, BatchNormalization, GlobalAveragePooling2D, Dropout,Flatten\n",
    "from keras.models import Model\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.optimizers import Adamax\n",
    "from keras.optimizers import Nadam\n",
    "\n",
    "\n",
    "def getVggModel():\n",
    "    \n",
    "    # VGG16换成其他模型？？\n",
    "    base_model = DenseNet121(weights='imagenet', include_top=False, \n",
    "                 input_shape=X_train.shape[1:], classes=1)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "#     x = GlobalMaxPooling2D()(x)\n",
    "#     x=Flatten()(x)\n",
    "#     x = (x-np.mean(x))/(np.max(x)-np.min(x))\n",
    "    \n",
    "    # mobile net\n",
    "#     base_model2 = keras.applications.mobilenet.MobileNet(weights=None, alpha=0.9,input_tensor = base_model.input,include_top=False, input_shape=X_train.shape[1:])\n",
    "#     base_model2 = keras.applications.mobilenet.MobileNet(weights=None, alpha=0.9,\n",
    "#                                                          include_top=False, input_shape=X_train.shape[1:])\n",
    "#     base_model2 = Xception(weights='imagenet', include_top=False, input_tensor = base_model.input,\n",
    "#                  input_shape=X_train.shape[1:], classes=1)\n",
    "#     x2 = base_model2.output\n",
    "#     x2 = GlobalMaxPooling2D()(x2)\n",
    "        \n",
    "    merge_one = x\n",
    "    merge_one = Dense(512, activation='relu', name='fc2')(merge_one)#原来\n",
    "#     merge_one = Dense(1024, activation='relu', name='fc2')(merge_one)\n",
    "    merge_one = Dropout(0.3)(merge_one) # 参数原来0.3\n",
    "    merge_one = Dense(512, activation='relu', name='fc3')(merge_one)\n",
    "#     merge_one = Dense(256, activation='relu', name='fc3')(merge_one)\n",
    "    merge_one = Dropout(0.3)(merge_one)\n",
    "    \n",
    "    predictions = Dense(1, activation='sigmoid')(merge_one)\n",
    "    \n",
    "    model = Model(input=base_model.input, output=predictions)\n",
    "    \n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # 使用不同的优化\n",
    "    sgd = SGD(lr=5e-4, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    adagrad = Adagrad(lr = 1e-3, epsilon = 1e-6)\n",
    "    rmsprop = RMSprop(lr=1e-3, rho = 0.9, epsilon=1e-6)\n",
    "    adadelta = Adadelta(lr=1e-3, rho=0.95, epsilon=1e-06)\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    adamax = Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    nadam = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "    \n",
    "    # 更换loss\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 自定义损失函数\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# FROM https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/41108\n",
    "def jacek_auc(y_true, y_pred):\n",
    "#     score, up_opt = tf.metrics.auc(y_true, y_pred)\n",
    "    score, up_opt = tf.contrib.metrics.streaming_auc(y_pred, y_true)    \n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    with tf.control_dependencies([up_opt]):\n",
    "        score = tf.identity(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "batch_size = 16 # 原来是3\n",
    "\n",
    "#Lets define the image transormations that we want\n",
    "gen = ImageDataGenerator(horizontal_flip=True,\n",
    "                         vertical_flip=True,\n",
    "                         width_shift_range=0.05,\n",
    "                         height_shift_range=0.05,\n",
    "                         zoom_range=0.2,\n",
    "                         rotation_range=10,\n",
    "                         shear_range = 0.05)\n",
    "\n",
    "# Here is the function that merges our two generators\n",
    "# We use the exact same generator with the same random seed for both the y and angle arrays\n",
    "def gen_flow_for_one_input(X1, y):\n",
    "    genX1 = gen.flow(X1,y,  batch_size=batch_size,seed=2018)\n",
    "    while True:\n",
    "            X1i = genX1.next()\n",
    "            #Assert arrays are equal - this was for peace of mind, but slows down training\n",
    "            #np.testing.assert_array_equal(X1i[0],X2i[0])\n",
    "            yield X1i[0], X1i[1]\n",
    "\n",
    "#Finally create out generator\n",
    "# gen_flow = gen_flow_for_one_inputs(X_train, y_train)\n",
    "\n",
    "# Finally create generator\n",
    "def get_callbacks(filepath, patience=2):\n",
    "   es = EarlyStopping('val_loss', patience=10, mode=\"min\")\n",
    "   msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "   return [es, msave]\n",
    "\n",
    "'''\n",
    "epochs_to_wait_for_improve = 10\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=epochs_to_wait_for_improve)\n",
    "checkpoint_callback = ModelCheckpoint('./model/BestKerasModelResNet50.h5', monitor='val_loss', \n",
    "                                      verbose=1, save_best_only=True, mode='min')\n",
    "'''\n",
    "\n",
    "#Using K-fold Cross Validation with Data Augmentation.\n",
    "def mytrainCV(X_train, X_test):\n",
    "    # K-折交叉验证\n",
    "    K=3\n",
    "    \n",
    "    folds = list(StratifiedKFold(n_splits=K, shuffle=True, random_state=2016).split(X_train, target_train))\n",
    "    y_test_pred_log = 0\n",
    "    y_train_pred_log=0\n",
    "    y_valid_pred_log = 0.0*target_train\n",
    "    \n",
    "    auc = 0;\n",
    "    \n",
    "    for j, (train_idx, test_idx) in enumerate(folds):\n",
    "        print('\\n===================FOLD=',j)\n",
    "        X_train_cv = X_train[train_idx]\n",
    "        y_train_cv = target_train[train_idx]\n",
    "        X_holdout = X_train[test_idx]\n",
    "        Y_holdout= target_train[test_idx]\n",
    "        \n",
    "\n",
    "        #define file path and get callbacks\n",
    "        file_path = \"./model/%s_aug_densenet_model_weights.hdf5\"%j\n",
    "        callbacks = get_callbacks(filepath=file_path, patience=5)\n",
    "        gen_flow = gen_flow_for_one_input(X_train_cv, y_train_cv)\n",
    "        gen_flow_cv = gen_flow_for_one_input(X_holdout, Y_holdout)\n",
    "\n",
    "        galaxyModel= getVggModel()\n",
    "    \n",
    "        # 调整训练参数\n",
    "        galaxyModel.fit_generator(\n",
    "                gen_flow,\n",
    "                steps_per_epoch=len(X_train_cv)//batch_size,\n",
    "                #steps_per_epoch=100,\n",
    "                epochs=100,\n",
    "                shuffle=True,\n",
    "                verbose=1,\n",
    "#                 validation_data=gen_flow_cv,\n",
    "                validation_data=(X_holdout, Y_holdout),\n",
    "                callbacks=callbacks)\n",
    "\n",
    "        #Getting the Best Model\n",
    "        galaxyModel.load_weights(filepath=file_path)\n",
    "        \n",
    "        #Getting Training Score\n",
    "        score = galaxyModel.evaluate(X_train_cv, y_train_cv, verbose=0)\n",
    "        print('Train loss:', score[0])\n",
    "        print('Train accuracy:', score[1])\n",
    "        \n",
    "        #Getting Test Score\n",
    "        score = galaxyModel.evaluate(X_holdout, Y_holdout, verbose=0)\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "\n",
    "        #Getting validation Score.       \n",
    "        pred_valid=galaxyModel.predict(X_holdout)\n",
    "        y_valid_pred_log[test_idx] = pred_valid.reshape(pred_valid.shape[0])\n",
    "\n",
    "        #Getting Test Scores\n",
    "        temp_test=galaxyModel.predict(X_test)\n",
    "        y_test_pred_log+=temp_test.reshape(temp_test.shape[0])\n",
    "\n",
    "        #Getting Train Scores        \n",
    "        temp_train=galaxyModel.predict(X_train)\n",
    "        y_train_pred_log+=temp_train.reshape(temp_train.shape[0])\n",
    "        \n",
    "        # AUC \n",
    "        auc_temp = roc_auc_score(Y_holdout,pred_valid)\n",
    "        print(\"AUC = {0:0.4f}\".format(auc_temp))\n",
    "        \n",
    "        auc+=auc_temp\n",
    "        \n",
    "    y_test_pred_log=y_test_pred_log/K\n",
    "    y_train_pred_log=y_train_pred_log/K\n",
    "    auc = auc/K\n",
    "\n",
    "    print('\\n Train Loss Validation= ',log_loss(target_train, y_train_pred_log))\n",
    "    print(' Test Loss Validation= ',log_loss(target_train, y_valid_pred_log))\n",
    "    print('AUC Validation=',auc)\n",
    "    return y_test_pred_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================FOLD= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jayden/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:49: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "84/84 [==============================] - 67s 793ms/step - loss: 0.6700 - acc: 0.6071 - val_loss: 0.5949 - val_acc: 0.6978\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 56s 665ms/step - loss: 0.6111 - acc: 0.6778 - val_loss: 0.5547 - val_acc: 0.7289\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 55s 660ms/step - loss: 0.5789 - acc: 0.7133 - val_loss: 0.5167 - val_acc: 0.7541\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 56s 662ms/step - loss: 0.5518 - acc: 0.7341 - val_loss: 0.4963 - val_acc: 0.7704\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 55s 661ms/step - loss: 0.5585 - acc: 0.7315 - val_loss: 0.5051 - val_acc: 0.7644\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 56s 661ms/step - loss: 0.5277 - acc: 0.7515 - val_loss: 0.4962 - val_acc: 0.7733\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 56s 661ms/step - loss: 0.5105 - acc: 0.7797 - val_loss: 0.5078 - val_acc: 0.7585\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 56s 661ms/step - loss: 0.5035 - acc: 0.7728 - val_loss: 0.5286 - val_acc: 0.7689\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 56s 661ms/step - loss: 0.4910 - acc: 0.7885 - val_loss: 0.4992 - val_acc: 0.7704\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 56s 661ms/step - loss: 0.4758 - acc: 0.7847 - val_loss: 0.5272 - val_acc: 0.7615\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 56s 662ms/step - loss: 0.4614 - acc: 0.8048 - val_loss: 0.5698 - val_acc: 0.7393\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 56s 662ms/step - loss: 0.4570 - acc: 0.8050 - val_loss: 0.5196 - val_acc: 0.7689\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 56s 661ms/step - loss: 0.4455 - acc: 0.8041 - val_loss: 0.5081 - val_acc: 0.7778\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 56s 662ms/step - loss: 0.4398 - acc: 0.7974 - val_loss: 0.4971 - val_acc: 0.7911\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 56s 662ms/step - loss: 0.4250 - acc: 0.8169 - val_loss: 0.5536 - val_acc: 0.7674\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 56s 661ms/step - loss: 0.4282 - acc: 0.7987 - val_loss: 0.5240 - val_acc: 0.7733\n",
      "Train loss: 0.4644396889183197\n",
      "Train accuracy: 0.791388270274391\n",
      "Test loss: 0.4961572011311849\n",
      "Test accuracy: 0.7733333333774849\n",
      "AUC = 0.8023\n",
      "\n",
      "===================FOLD= 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "preds=mytrainCV(X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Submission for each day.\n",
    "submission = pd.DataFrame()\n",
    "submission['filename']=filename_data\n",
    "submission['probability']=preds\n",
    "submission.to_csv('./submission/densenet2.0.csv',float_format='%.6f',index=False)\n",
    "# submission.to_csv('./submission/subVgg2.0.csv', \n",
    "#                   float_format='%.6f',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
